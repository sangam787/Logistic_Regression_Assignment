{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwEAHfauZ3mb"
      },
      "source": [
        "#**Logistic :-**\n",
        "\n",
        "#**Assignment Questions**\n",
        "\n",
        "#**Theoretical:-**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bqwYatoZ5lS"
      },
      "source": [
        "#Que-1What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "#**Ans-1**\n",
        "\n",
        "Logistic Regression is a statistical method used for classification tasks, where the goal is to predict the probability of a data point belonging to a specific class. It's a type of supervised learning algorithm, meaning it learns from labeled data to make predictions.\n",
        "\n",
        "**How does it differ from Linear Regression?**\n",
        "\n",
        "\n",
        "\n",
        "1. Purpose:\n",
        "\n",
        " * Linear Regression: Predicts a continuous output variable (e.g., predicting house prices).\n",
        " * Logistic Regression: Predicts a categorical or discrete output variable (e.g., classifying emails as spam or not spam).\n",
        "\n",
        "\n",
        "2. Output:\n",
        "\n",
        " * Linear Regression: Output is a continuous value.\n",
        " * Logistic Regression: Output is a probability between 0 and 1, representing the likelihood of belonging to a specific class.\n",
        "\n",
        "3. Model:\n",
        "\n",
        " * Linear Regression: Uses a linear equation to model the relationship between variables.\n",
        " * Logistic Regression: Uses a sigmoid function to transform the linear equation's output into a probability.\n",
        "\n",
        "4. Cost Function:\n",
        "\n",
        " * Linear Regression: Uses mean squared error to measure the difference between predicted and actual values.\n",
        " * Logistic Regression: Uses log loss (or cross-entropy) to measure the error in probability predictions.\n",
        "\n",
        "5. Evaluation Metrics:\n",
        "\n",
        " * Linear Regression: Uses metrics like R-squared, mean absolute error, etc.\n",
        " * Logistic Regression: Uses metrics like accuracy, precision, recall, F1-score, ROC-AUC, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQDjldhZZ5r2"
      },
      "source": [
        "#Que-2 What is the mathematical equation of Logistic Regression?\n",
        "#**Ans-2**\n",
        "\n",
        "The core of Logistic Regression lies in the sigmoid function, also known as the logistic function. This function takes any real-valued input and maps it to an output between 0 and 1, representing a probability.\n",
        "\n",
        "Here's the equation:\n",
        "\n",
        "\n",
        "p = 1 / (1 + e^(-z))\n",
        "\n",
        "where:\n",
        "\n",
        "* p is the predicted probability of the data point belonging to the positive class (class 1).\n",
        "* e is the base of the natural logarithm (approximately 2.718).\n",
        "* z is the linear combination of the input features and their corresponding weights, represented as:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAo0B0jJZ5yJ"
      },
      "source": [
        "#Que-3 Why do we use the Sigmoid function in Logistic Regression?\n",
        "#**Ans-3**\n",
        "\n",
        "The Sigmoid function, also known as the logistic function, plays a central role in Logistic Regression for the following reasons:\n",
        "\n",
        "1. Probability Mapping: The primary purpose of Logistic Regression is to predict the probability of a data point belonging to a specific class. The Sigmoid function takes any real-valued input (the result of the linear equation) and maps it to an output between 0 and 1, which represents a probability. This mapping ensures that the model's output is always within the valid range for probabilities.\n",
        "\n",
        "2. Decision Boundary: The Sigmoid function helps create a smooth, non-linear decision boundary between classes. In classification tasks, we aim to separate data points into distinct categories. The Sigmoid function's S-shaped curve allows for a gradual transition between classes, rather than a sharp, abrupt cutoff. This smooth boundary is often more suitable for real-world data.\n",
        "\n",
        "3. Differentiability: The Sigmoid function is differentiable, meaning it has a derivative that can be calculated. This property is crucial for training the Logistic Regression model using optimization algorithms like gradient descent. The derivative of the Sigmoid function is used to update the model's weights and improve its predictions.\n",
        "\n",
        "4. Interpretability: The output of the Sigmoid function can be interpreted as the probability of the positive class (class 1). This interpretability is valuable for understanding the model's predictions and assessing their confidence levels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RZSVS_iZ536"
      },
      "source": [
        "#Que-4 What is the cost function of Logistic Regression?\n",
        "#**Ans-4**\n",
        "\n",
        "In Logistic Regression, the cost function is used to measure the error between the model's predictions and the actual values. The goal is to minimize this error during the training process to obtain an accurate model.\n",
        "\n",
        "The cost function for Logistic Regression is called the Log Loss or Cross-Entropy Loss. Here's the formula:\n",
        "\n",
        "\n",
        "Cost(hθ(x), y) = -y * log(hθ(x)) - (1 - y) * log(1 - hθ(x))\n",
        "\n",
        "Where:\n",
        "\n",
        "* hθ(x) is the model's prediction (probability of the positive class) for a given input x.\n",
        "* y is the actual class label (0 or 1)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sa4zWocRZ5-E"
      },
      "source": [
        "#Que-5 What is Regularization in Logistic Regression? Why is it needed?\n",
        "#**Ans-5**\n",
        "\n",
        "Regularization is a technique used in machine learning to prevent overfitting, which occurs when a model learns the training data too well and performs poorly on unseen data. It does this by adding a penalty term to the cost function, discouraging the model from assigning excessively large weights to features.\n",
        "\n",
        "\n",
        "In Logistic Regression, regularization is particularly important for the following reasons:\n",
        "\n",
        "1. Preventing Overfitting: When a Logistic Regression model is overfit, it may capture noise or random fluctuations in the training data as patterns, leading to poor generalization to new data. Regularization helps to smooth out the decision boundary and prevent the model from memorizing the training set.\n",
        "\n",
        "2. Handling Multicollinearity: Multicollinearity occurs when features are highly correlated with each other. This can make it difficult for the model to determine the individual importance of each feature. Regularization helps to stabilize the model and reduce the impact of multicollinearity.\n",
        "\n",
        "3. Improving Model Generalization: By preventing overfitting and handling multicollinearity, regularization improves the model's ability to generalize well to unseen data, making it more robust and reliable in real-world scenarios."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4R4BZRtRZ6ED"
      },
      "source": [
        "#Que-6 Explain the difference between Lasso, Ridge, and Elastic Net regression.\n",
        "#**Ans-6**\n",
        "\n",
        "These three techniques are all forms of regularization, a crucial concept in machine learning used to prevent overfitting and improve model generalization. They achieve this by adding a penalty term to the loss function, discouraging the model from assigning excessively large weights to features.\n",
        "\n",
        "Here's a breakdown of their differences:\n",
        "\n",
        " 1. **Lasso Regression (L1 Regularization):**\n",
        "\n",
        "* Penalty Term: Adds a penalty proportional to the absolute values of the coefficients (L1 penalty).\n",
        "* Effect: Shrinks some coefficients to exactly zero, effectively performing feature selection. This leads to a sparse model with fewer features.\n",
        "* Benefits:\n",
        " * Automatic feature selection, simplifying the model.\n",
        " * Can handle datasets with more features than observations.\n",
        "* Drawbacks:\n",
        " * May randomly select one feature among a group of highly correlated features.\n",
        " * Can be unstable with highly correlated features.\n",
        "\n",
        "2. **Ridge Regression (L2 Regularization):**\n",
        "\n",
        "* Penalty Term: Adds a penalty proportional to the square of the coefficients (L2 penalty).\n",
        "* Effect: Shrinks coefficients towards zero, but rarely to exactly zero. This reduces the impact of all features without eliminating any.\n",
        "* Benefits:\n",
        " * Handles multicollinearity (high correlation between features) better than Lasso.\n",
        " * Generally produces more stable models.\n",
        "* Drawbacks:\n",
        " * Doesn't perform feature selection, keeping all features in the model.\n",
        " * May not be as effective as Lasso in highly sparse datasets.\n",
        "\n",
        "3. **Elastic Net Regression:**\n",
        "\n",
        "* Penalty Term: Combines both L1 and L2 penalties.\n",
        "* Effect: Balances feature selection (from Lasso) and coefficient shrinkage (from Ridge).\n",
        "* Benefits:\n",
        " * Enjoys the benefits of both Lasso and Ridge.\n",
        " * Handles multicollinearity well and performs feature selection.\n",
        " * More stable and robust than Lasso in many cases.\n",
        "* Drawbacks:\n",
        " * Introduces an additional hyperparameter to tune (the mixing parameter between L1 and L2 penalties)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQE_ae2XZ6KZ"
      },
      "source": [
        "#Que-7 When should we use Elastic Net instead of Lasso or Ridge?\n",
        "#**Ans-7**\n",
        "\n",
        "Use Elastic Net when:\n",
        "\n",
        "1. You have a dataset with many features, some of which are highly correlated. Elastic Net combines the strengths of both Lasso and Ridge, making it a good choice in this scenario. Lasso tends to arbitrarily select one feature from a group of correlated features, while Ridge shrinks all their coefficients towards zero. Elastic Net strikes a balance by performing feature selection (like Lasso) while also handling multicollinearity effectively (like Ridge).\n",
        "\n",
        "2. You want a model that is both sparse and stable. Lasso can create very sparse models by setting some coefficients to zero, but it can be unstable if there are many correlated features. Ridge produces more stable models, but they may not be as sparse. Elastic Net offers a compromise between sparsity and stability.\n",
        "\n",
        "3. You're unsure whether to use Lasso or Ridge. Elastic Net can be a good option if you're not sure which type of regularization is best for your data. By combining both L1 and L2 penalties, it offers a more flexible approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7zvywmlZ6Qp"
      },
      "source": [
        "#Que-8 What is the impact of the regularization parameter (λ) in Logistic Regression?\n",
        "#**Ans-8**\n",
        "\n",
        "Impact of λ on Model Complexity and Overfitting\n",
        "\n",
        "* High λ (or small C): A high value of λ (or small C) imposes a strong penalty on large coefficients. This leads to:\n",
        "\n",
        " * Simpler Models: The model is encouraged to have smaller weights, effectively shrinking the influence of individual features.\n",
        " * Reduced Overfitting: The model is less likely to overfit the training data by capturing noise or random fluctuations.\n",
        " * Increased Bias: However, with excessive regularization, the model might underfit the data and not capture important patterns.\n",
        " * Example: Imagine using L2 regularization. If λ is high, the cost function is heavily penalized for large weights. During optimization, the model will prioritize minimizing this penalty, leading to smaller weight values overall.\n",
        "\n",
        "* Low λ (or large C): A low value of λ (or large C) imposes a weak penalty on large coefficients. This results in:\n",
        "\n",
        " * Complex Models: The model can have larger weights, allowing it to fit the training data more closely.\n",
        " * Increased Risk of Overfitting: However, with less regularization, the model is more likely to overfit the training data.\n",
        " * Reduced Bias: The model is less prone to underfitting but might capture noise or irrelevant details.\n",
        " * Example: Consider L1 regularization. If λ is low, the penalty for large weights is minimal. The model will try to fit the training data as closely as possible, potentially leading to overfitting if there's noise or outliers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaFc3ivyZ6Wp"
      },
      "source": [
        "#Que-9 What are the key assumptions of Logistic Regression?\n",
        "#**Ans-9**\n",
        "\n",
        "Logistic Regression, while a powerful and widely used algorithm, relies on certain assumptions for its results to be valid and reliable. Here are the key assumptions:\n",
        "\n",
        "1. Linearity in the Logit: Logistic Regression assumes a linear relationship between the independent variables and the log-odds (logit) of the dependent variable. The logit is the natural logarithm of the odds of the event occurring. In simpler terms, it assumes that a change in the predictor variables will have a proportional impact on the log-odds of the outcome.\n",
        "\n",
        "2. Independence of Errors: The errors (or residuals) should be independent of each other. This means that the error for one observation should not be related to the error for another observation. This assumption is important to ensure that the model's estimates are unbiased.\n",
        "\n",
        "3. No Multicollinearity: There should be little or no multicollinearity among the independent variables. Multicollinearity occurs when two or more predictor variables are highly correlated with each other. This can make it difficult to estimate the individual effects of each variable, leading to unstable and unreliable coefficients.\n",
        "\n",
        "4. No Outliers: The dataset should be free of extreme outliers that can significantly influence the model's estimates. Outliers can distort the relationship between the independent and dependent variables and lead to biased results.\n",
        "\n",
        "5. Large Sample Size: Logistic Regression generally performs better with larger datasets. A sufficient sample size helps ensure that the model's estimates are stable and reliable, especially when there are many predictor variables.\n",
        "\n",
        "6. Binary Outcome: Traditional Logistic Regression is designed for binary classification problems, where the dependent variable has only two possible outcomes (e.g., yes/no, 0/1). Although there are extensions for multi-class classification (like multinomial or ordinal logistic regression), the basic assumption is that the outcome is binary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BU-npwkqZ6cJ"
      },
      "source": [
        "#Que-10 What are some alternatives to Logistic Regression for classification tasks?\n",
        "#**Ans-10**\n",
        "\n",
        "While Logistic Regression is a popular and effective algorithm for classification, there are several other methods that might be more suitable depending on the specific problem and dataset. Here are some alternatives:\n",
        "\n",
        "1. Support Vector Machines (SVM)\n",
        "\n",
        "* How they work: SVMs aim to find the best hyperplane that separates data points into different classes with the largest margin. They can handle both linear and non-linear classification tasks using kernel functions.\n",
        "* Strengths: Effective in high-dimensional spaces, versatile due to kernel functions, and robust to outliers.\n",
        "* Weaknesses: Can be computationally expensive for large datasets and sensitive to the choice of kernel function.\n",
        "2. Decision Trees\n",
        "\n",
        "* How they work: Decision Trees create a tree-like structure to classify data points based on a series of decisions made on feature values.\n",
        "* Strengths: Easy to understand and interpret, can handle both categorical and numerical data, and require minimal data preparation.\n",
        "* Weaknesses: Prone to overfitting, can be unstable if the data changes slightly, and may not perform well with complex relationships.\n",
        "3. Random Forests\n",
        "\n",
        "* How they work: Random Forests build an ensemble of decision trees and combine their predictions to improve accuracy and reduce overfitting.\n",
        "* Strengths: Robust to overfitting, handle high-dimensional data well, and can identify important features.\n",
        "* Weaknesses: Can be computationally expensive and less interpretable than single decision trees.\n",
        "4. Naive Bayes\n",
        "\n",
        "* How they work: Naive Bayes classifiers are based on Bayes' theorem and assume that features are independent of each other.\n",
        "* Strengths: Simple and fast, perform well with high-dimensional data, and effective for text classification tasks.\n",
        "* Weaknesses: The independence assumption is often unrealistic in real-world scenarios.\n",
        "5. K-Nearest Neighbors (KNN)\n",
        "\n",
        "* How they work: KNN classifies data points based on the class of their nearest neighbors in the feature space.\n",
        "* Strengths: Easy to understand and implement, no training phase required (lazy learner), and can handle non-linear decision boundaries.\n",
        "* Weaknesses: Can be computationally expensive for large datasets and sensitive to the choice of distance metric.\n",
        "6. Neural Networks\n",
        "\n",
        "* How they work: Neural Networks consist of interconnected nodes organized in layers that learn complex patterns from data.\n",
        "* Strengths: Can learn very complex relationships, highly flexible, and achieve state-of-the-art performance in many tasks.\n",
        "* Weaknesses: Can be computationally expensive, require large amounts of data, and often considered a \"black box\" due to their complexity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JX4Su3SuZ6iZ"
      },
      "source": [
        "#Que-11 What are Classification Evaluation Metrics?\n",
        "#**Ans-11**\n",
        "\n",
        "Classification evaluation metrics are used to assess the performance of a classification model. They provide a way to quantify how well the model is able to predict the correct class labels for new, unseen data.\n",
        "\n",
        "Here are some commonly used classification evaluation metrics:\n",
        "\n",
        "1. Accuracy\n",
        "\n",
        "* Definition: The proportion of correctly classified instances out of the total number of instances.\n",
        "* Formula: (Number of correct predictions) / (Total number of predictions)\n",
        "* Interpretation: A higher accuracy indicates a better-performing model. However, it can be misleading when dealing with imbalanced datasets (where one class is much more prevalent than others).\n",
        "2. Precision\n",
        "\n",
        "* Definition: The proportion of true positive predictions out of all positive predictions made by the model.\n",
        "* Formula: (True Positives) / (True Positives + False Positives)\n",
        "* Interpretation: Precision measures the model's ability to avoid making false positive predictions. A higher precision indicates fewer false positives.\n",
        "3. Recall (Sensitivity)\n",
        "\n",
        "* Definition: The proportion of true positive predictions out of all actual positive instances in the dataset.\n",
        "* Formula: (True Positives) / (True Positives + False Negatives)\n",
        "* Interpretation: Recall measures the model's ability to correctly identify all positive instances. A higher recall indicates fewer false negatives.\n",
        "4. F1-Score\n",
        "\n",
        "* Definition: The harmonic mean of precision and recall, providing a balanced measure of both metrics.\n",
        "Formula: 2 * (Precision * Recall) / (Precision + Recall)\n",
        "* Interpretation: The F1-score is a useful metric when you want to consider both precision and recall, especially when dealing with imbalanced datasets.\n",
        "5. ROC-AUC (Receiver Operating Characteristic - Area Under the Curve)\n",
        "\n",
        "* Definition: A measure of the model's ability to distinguish between positive and negative classes across different classification thresholds.\n",
        "* Interpretation: A higher ROC-AUC score indicates better discrimination between classes. A score of 1 represents a perfect classifier, while a score of 0.5 represents a random classifier.\n",
        "6. Confusion Matrix\n",
        "\n",
        "* Definition: A table that summarizes the performance of a classification model by showing the counts of true positives, true negatives, false positives, and false negatives.\n",
        "* Interpretation: The confusion matrix provides a detailed view of the model's performance, allowing you to identify specific types of errors it makes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0PG6N8WZ6n5"
      },
      "source": [
        "#Que-12 How does class imbalance affect Logistic Regression?\n",
        "#**Ans-12**\n",
        "\n",
        "Class imbalance occurs when one class (the majority class) has significantly more instances than another class (the minority class) in a dataset. This imbalance can pose challenges for Logistic Regression, as the model might become biased towards the majority class and perform poorly on the minority class, which is often the class of interest.\n",
        "\n",
        "Here's how class imbalance affects Logistic Regression:\n",
        "\n",
        "1. Bias towards the Majority Class: Logistic Regression aims to minimize the overall error, and with imbalanced data, it can achieve this by simply predicting the majority class most of the time. This leads to high accuracy but poor performance on the minority class.\n",
        "\n",
        "2. Inaccurate Probability Estimates: The model might produce inaccurate probability estimates for the minority class. Since there are fewer examples of the minority class, the model has less information to learn its characteristics and might underestimate its probability.\n",
        "\n",
        "3. Poor Generalization on the Minority Class: The model may not generalize well to new, unseen data, especially for the minority class. It might misclassify minority class instances as the majority class, leading to a high rate of false negatives."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZ8CyqgXZ6ur"
      },
      "source": [
        "#Que-13 What is Hyperparameter Tuning in Logistic Regression?\n",
        "#**Ans-13**\n",
        "\n",
        "Hyperparameters in Logistic Regression\n",
        "\n",
        "Some important hyperparameters in Logistic Regression include:\n",
        "\n",
        "* Regularization parameter (C or penalty): Controls the strength of regularization (L1, L2, or Elastic Net). A higher value of C means less regularization, while a lower value means stronger regularization.\n",
        "* Solver: The algorithm used for optimization (e.g., 'liblinear', 'lbfgs', 'saga'). Different solvers have different strengths and weaknesses and are suitable for different types of datasets.\n",
        "* tol (Tolerance for stopping criteria): Defines when the optimization algorithm should stop iterating.\n",
        "* max_iter (Maximum number of iterations): Limits the number of iterations the optimization algorithm can perform."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxxN_cWxZ61l"
      },
      "source": [
        "#Que-14 What are different solvers in Logistic Regression? Which one should be used?\n",
        "#**Ans-14**\n",
        "\n",
        "In Logistic Regression, solvers are optimization algorithms used to find the best-fitting model parameters (coefficients) that minimize the cost function. Different solvers have different characteristics and are suitable for different types of datasets and problem settings.\n",
        "\n",
        "Here are some commonly used solvers in Logistic Regression:\n",
        "\n",
        "1. liblinear:\n",
        "\n",
        " * Uses a coordinate descent algorithm.\n",
        " * Suitable for small to medium-sized datasets.\n",
        " * Efficient for L1 and L2 regularization.\n",
        " * Supports both binary and multi-class classification.\n",
        "2. lbfgs:\n",
        "\n",
        " * Implements a limited-memory Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm.\n",
        " * Suitable for larger datasets and problems with smooth cost functions.\n",
        " * Efficient for L2 regularization but does not support L1 regularization.\n",
        " * Supports only binary classification (use OneVsRestClassifier or OneVsOneClassifier for multi-class).\n",
        "3. newton-cg:\n",
        "\n",
        " * Uses a Newton-Conjugate Gradient algorithm.\n",
        " * Suitable for larger datasets and problems with smooth cost functions.\n",
        " * Efficient for L2 regularization but does not support L1 regularization.\n",
        " * Supports only binary classification (use OneVsRestClassifier or OneVsOneClassifier for multi-class).\n",
        "4. sag:\n",
        "\n",
        " * Implements a Stochastic Average Gradient descent algorithm.\n",
        " * Suitable for very large datasets.\n",
        " * Efficient for L2 regularization but does not support L1 regularization.\n",
        " * Supports only binary classification (use OneVsRestClassifier or OneVsOneClassifier for multi-class).\n",
        "\n",
        "5. saga:\n",
        "\n",
        " * A variant of sag that supports L1 regularization.\n",
        " * Suitable for very large datasets and for both L1 and L2 regularization.\n",
        " * Supports both binary and multi-class classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWA8WN5CZ6-B"
      },
      "source": [
        "#Que-15 How is Logistic Regression extended for multiclass classification?\n",
        "#**Ans-15**\n",
        "\n",
        "While traditional Logistic Regression is designed for binary classification (two classes), it can be extended to handle multiclass problems where the dependent variable has more than two categories. There are two primary approaches for this extension:\n",
        "\n",
        "1. One-vs-Rest (OvR) or One-vs-All (OvA)\n",
        "\n",
        "* How it works: In OvR, a separate binary Logistic Regression model is trained for each class. Each model learns to distinguish between a specific class and all other classes. During prediction, the model that assigns the highest probability to a particular class is selected, and that class is assigned to the input data point.\n",
        "* Example: If you have three classes (A, B, C), you would train three models:\n",
        " * Model 1: Distinguishes between class A and classes B & C\n",
        " * Model 2: Distinguishes between class B and classes A & C\n",
        " * Model 3: Distinguishes between class C and classes A & B\n",
        "2. Multinomial Logistic Regression (Softmax Regression)\n",
        "\n",
        "* How it works: Multinomial Logistic Regression directly models the probability of an instance belonging to each class using a softmax function. The softmax function outputs a probability distribution over all classes, ensuring that the probabilities sum to 1. The class with the highest probability is assigned to the input data point.\n",
        "* Difference from OvR: Unlike OvR, which treats each class independently, Multinomial Logistic Regression considers all classes simultaneously when making predictions. This can lead to better performance in some cases, especially when the classes are correlated.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLpHYGgiZ7Eh"
      },
      "source": [
        "#Que-16 What are the advantages and disadvantages of Logistic Regression?\n",
        "#**Ans-16**\n",
        "\n",
        "Advantages of Logistic Regression\n",
        "\n",
        "1. Simplicity and Interpretability: Logistic Regression is relatively easy to understand and implement. The model's coefficients can be interpreted as the change in the log-odds of the outcome for a one-unit change in the predictor variable. This interpretability makes it valuable for understanding the relationships between variables.\n",
        "\n",
        "2. Efficiency: Logistic Regression is computationally efficient, especially for smaller datasets. It trains quickly and requires relatively low memory, making it suitable for applications where speed is crucial.\n",
        "\n",
        "3. Probability Estimates: Logistic Regression provides probability estimates for each class, which can be useful for understanding the model's confidence in its predictions and for making informed decisions based on those probabilities.\n",
        "\n",
        "4. Feature Importance: The model's coefficients can be used to identify the most important features for predicting the outcome, providing insights into the factors driving the classification.\n",
        "\n",
        "5. Regularization: Logistic Regression can incorporate regularization techniques (L1, L2, or Elastic Net) to prevent overfitting and improve generalization performance.\n",
        "\n",
        "6. Widely Used and Well-Understood: Logistic Regression is a well-established algorithm with extensive theoretical support and a large user base. This means that there are many resources and tools available for working with it.\n",
        "\n",
        "Disadvantages of Logistic Regression\n",
        "\n",
        "1. Linearity Assumption: Logistic Regression assumes a linear relationship between the independent variables and the log-odds of the dependent variable. This assumption might not hold for all datasets, and non-linear relationships might need to be addressed using feature engineering or other models.\n",
        "\n",
        "2. Sensitivity to Outliers: Logistic Regression can be sensitive to outliers, which can significantly influence the model's coefficients and predictions. Outlier detection and treatment might be necessary.\n",
        "\n",
        "3. Limited Complexity: Logistic Regression is a relatively simple model and might not be able to capture complex relationships or interactions between variables. More complex models like neural networks might be needed for such cases.\n",
        "\n",
        "4. Multicollinearity: Logistic Regression can be affected by multicollinearity (high correlation between predictor variables). This can lead to unstable coefficient estimates and difficulties in interpreting feature importance.\n",
        "\n",
        "5. Requires Large Datasets: While Logistic Regression can work with smaller datasets, it generally performs better with larger datasets to ensure reliable coefficient estimates and generalization performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSZLhgKvZ7Jx"
      },
      "source": [
        "#Que-17 What are some use cases of Logistic Regression?\n",
        "#**Ans-17**\n",
        "\n",
        "Logistic Regression is a versatile algorithm used in various fields for binary and multiclass classification tasks. Here are some common use cases:\n",
        "\n",
        "1. Medical Diagnosis:\n",
        "\n",
        "* Predicting Disease Risk: Logistic Regression can be used to predict the likelihood of a patient developing a particular disease based on their medical history, symptoms, and risk factors.\n",
        "* Identifying Patients at High Risk: It can help identify individuals at higher risk of developing certain conditions, enabling early intervention and preventive measures.\n",
        "* Classifying Disease Severity: It can be used to classify the severity of a disease based on patient characteristics and test results.\n",
        "2. Financial Risk Assessment:\n",
        "\n",
        "* Credit Scoring: Logistic Regression is widely used in credit scoring to assess the creditworthiness of loan applicants and predict the probability of default.\n",
        "* Fraud Detection: It can help identify fraudulent transactions by analyzing patterns and anomalies in financial data.\n",
        "* Insurance Risk Prediction: It can be used to predict the likelihood of insurance claims based on customer profiles and policy details.\n",
        "3. Marketing and Customer Analytics:\n",
        "\n",
        "* Customer Churn Prediction: Logistic Regression can predict the likelihood of customers churning (leaving a service) based on their usage patterns, demographics, and feedback.\n",
        "* Targeted Advertising: It can help identify potential customers who are most likely to respond to a particular advertising campaign, enabling targeted marketing efforts.\n",
        "* Customer Segmentation: It can be used to segment customers into different groups based on their preferences and behavior, allowing businesses to personalize their offerings.\n",
        "4. Image Recognition and Natural Language Processing:\n",
        "\n",
        "* Spam Detection: Logistic Regression is commonly used in spam filters to classify emails as spam or not spam based on their content and sender information.\n",
        "* Sentiment Analysis: It can be used to analyze text data and classify the sentiment expressed as positive, negative, or neutral.\n",
        "* Object Recognition: It can be used in image recognition systems to identify and classify objects in images.\n",
        "5. Other Applications:\n",
        "\n",
        "* Predicting Election Outcomes: Logistic Regression has been used to predict the outcome of elections based on polling data and demographic information.\n",
        "* Assessing Student Performance: It can be used to predict student success or failure based on their academic records and other factors.\n",
        "* Classifying Documents: It can be used to categorize documents into different topics or genres based on their content."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQHS5bF2Z7Qx"
      },
      "source": [
        "#Que-18 What is the difference between Softmax Regression and Logistic Regression?\n",
        "#**Ans-18**\n",
        "\n",
        "Softmax Regression vs. Logistic Regression\n",
        "\n",
        "While both Softmax Regression and Logistic Regression are used for classification tasks, they differ primarily in the number of classes they can handle:\n",
        "\n",
        "* Logistic Regression: Is typically used for binary classification problems, where the outcome variable has only two possible classes (e.g., 0 or 1, spam or not spam).\n",
        "\n",
        "* Softmax Regression (Multinomial Logistic Regression): Is a generalization of Logistic Regression that can handle multiclass classification problems, where the outcome variable can have three or more possible classes (e.g., classifying images into categories like cats, dogs, birds)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJydYttEZ7XN"
      },
      "source": [
        "#Que-19 How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n",
        "#**Ans-19**\n",
        "\n",
        "proach and have different strengths and weaknesses.\n",
        "\n",
        "\n",
        "\n",
        "Consider OvR when:\n",
        "\n",
        "1. You have a large number of classes: OvR can be more computationally efficient than Softmax when dealing with many classes, as it trains independent binary classifiers for each class.\n",
        "2. Classes are not mutually exclusive: If there is some overlap or correlation between classes (e.g., an image could be tagged with multiple labels), OvR might be more suitable. It allows for instances to belong to multiple classes with different probabilities.\n",
        "3. You need interpretability: OvR is often easier to interpret than Softmax, as you can examine the coefficients of each individual binary classifier to understand the importance of features for each class.\n",
        "\n",
        "Consider Softmax when:\n",
        "\n",
        "1. Classes are mutually exclusive: If instances belong to only one class (e.g., classifying handwritten digits), Softmax is generally preferred. It provides a probability distribution over all classes, ensuring that the probabilities sum to 1.\n",
        "2. You need more accurate probability estimates: Softmax tends to produce better calibrated probability estimates than OvR, as it considers all classes simultaneously during training.\n",
        "3. Dataset is not too large: If your dataset is relatively small, the computational cost difference between OvR and Softmax might not be significant, and Softmax might be a better choice due to its potentially higher accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RRNwxKkZ7dB"
      },
      "source": [
        "#Que-20 How do we interpret coefficients in Logistic Regression?\n",
        "#**Ans-20**\n",
        "\n",
        "In Logistic Regression, the coefficients represent the change in the log-odds of the outcome for a one-unit change in the predictor variable, holding all other variables constant. To understand this better, let's break it down:\n",
        "\n",
        "1. Log-odds: The log-odds is the natural logarithm of the odds of the event occurring. It's a way to express the likelihood of an event in a continuous scale.\n",
        "\n",
        "2. Odds: The odds of an event is the ratio of the probability of the event occurring to the probability of the event not occurring. For example, if the probability of an event is 0.6, the odds are 0.6 / (1 - 0.6) = 1.5.\n",
        "\n",
        "3. Coefficients: The coefficients in Logistic Regression represent the change in the log-odds of the outcome for a one-unit change in the predictor variable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYJpReHuZ7je"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#**Practical-Questions:**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pr7h1nDDZ7o8"
      },
      "source": [
        "#Que-1 Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy.\n",
        "#**Ans-1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6gXXLuvd0z5",
        "outputId": "3073d124-80e3-4b8b-b8f3-310de76f5b75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# loading dataset\n",
        "\n",
        "data = pd.read_csv('/content/demodata.csv')  # Replace with your dataset file\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = data.drop('target', axis=1)  # Replace 'target_variable' with the actual target column name\n",
        "y = data['target']\n",
        "\n",
        "# spliting the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Creating and train the Logistic Regression Model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Making predictions on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculating and print the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"model Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-gmfpiEZ7vy"
      },
      "source": [
        "#Que-2 Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy.\n",
        "#**Ans-2**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9tPHYFX2mfA",
        "outputId": "52569a1d-f21a-41bd-9a5f-28992eeee4e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Accuracy with L1 Regularization: 0.81\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "np.random.seed(0)\n",
        "n_samples = 1000\n",
        "data = pd.DataFrame({\n",
        "    'customer_age': np.random.randint(20, 70, n_samples),\n",
        "    'monthly_bill': np.random.uniform(20, 150, n_samples),\n",
        "    'data_usage': np.random.uniform(0, 100, n_samples),\n",
        "    'contract_length': np.random.randint(1, 36, n_samples),\n",
        "    'online_support': np.random.randint(0, 2, n_samples)  # 0: No, 1: Yes\n",
        "})\n",
        "# Simulate churn based on some conditions (this is just example logic)\n",
        "data['churn'] = (data['monthly_bill'] > 100) | (data['contract_length'] < 6) | (data['data_usage'] > 80)\n",
        "data['churn'] = data['churn'].astype(int) # Convert True/False to 1/0\n",
        "\n",
        "\n",
        "# 2. Prepare data (features and target)\n",
        "X = data[['customer_age', 'monthly_bill', 'data_usage', 'contract_length', 'online_support']]\n",
        "y = data['churn']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Adjust test_size and random_state as needed\n",
        "\n",
        "# Create and train the Logistic Regression model with L1 regularization\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear')  # Use 'liblinear' solver for L1 penalty\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with L1 Regularization: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNz_S4E8Z72j"
      },
      "source": [
        "#Que-3 Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients.\n",
        "#**Ans-3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQtskQiM2yQl",
        "outputId": "d66380dd-e327-444a-bd59-d3412e407d33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Accuracy with L2 Regularization: 1.0\n",
            "Model Coefficients:\n",
            "[[1.71630167 0.0587587  0.18573873 0.49706417]]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "np.random.seed(0)\n",
        "data = pd.read_csv('/content/demodata.csv')  # Replace with your dataset file\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = data.drop('target', axis=1)  # Replace 'target_variable' with the actual target column name\n",
        "y = data['target']\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Adjust test_size and random_state as needed\n",
        "\n",
        "# Create and train the Logistic Regression model with L2 regularization\n",
        "model = LogisticRegression(penalty='l2')  # L2 regularization is the default\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with L2 Regularization: {accuracy}\")\n",
        "\n",
        "# Print the model coefficients\n",
        "print(\"Model Coefficients:\")\n",
        "print(model.coef_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7U2mS2t3Z7-p"
      },
      "source": [
        "#Que-4 Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet').\n",
        "#**Ans-4**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgg4bhUf2_hc",
        "outputId": "bf18fca6-7e08-46cc-bc94-0d41107dc69d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Accuracy with Elastic Net Regularization: 1.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/demodata.csv')  # Replace with your dataset file\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = data.drop('target', axis=1)  # Replace 'target_variable' with the actual target column name\n",
        "y = data['target']\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Adjust test_size and random_state as needed\n",
        "\n",
        "# Create and train the Logistic Regression model with Elastic Net regularization\n",
        "model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5)  # Use 'saga' solver for elasticnet\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with Elastic Net Regularization: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eE8JhZNOZ8GJ"
      },
      "source": [
        "#Que-5 Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'.\n",
        "#**Ans-5**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDQj69q_3Jz8",
        "outputId": "d02f866e-abb1-4ae7-9bf0-79a3b9fc29c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Accuracy (OvR): 0.31\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "n_samples = 1500  # Total number of samples\n",
        "n_features = 7    # Number of features\n",
        "n_classes = 3     # Number of classes for multiclass target variable\n",
        "# features (randomly generated for simplicity)\n",
        "np.random.seed(42) # for reproducibility\n",
        "features = np.random.randn(n_samples, n_features)\n",
        "feature_names = [f'feature_{i}' for i in range(n_features)]\n",
        "# Create multiclass target variable\n",
        "target = np.random.randint(0, n_classes, n_samples) # Randomly assign classes 0, 1, or 2\n",
        "# Pandas DataFrame\n",
        "df = pd.DataFrame(data=features, columns=feature_names)\n",
        "df['target'] = target.astype(int) # Ensure target is integer type\n",
        "df.to_csv('multiclass_dataset.csv', index=False)\n",
        "\n",
        "# Load the dataset (assuming it has a multiclass target variable)\n",
        "data = pd.read_csv('multiclass_dataset.csv')  # Replace with your dataset file\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = data.drop('target', axis=1)  # Replace 'target_variable' with the actual target column name\n",
        "y = data['target']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Logistic Regression model with 'ovr' strategy\n",
        "model = LogisticRegression(multi_class='ovr')  # Use 'ovr' for One-vs-Rest strategy\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy (OvR): {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hEGvOmyZ8NC"
      },
      "source": [
        "#Que-6 Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy.\n",
        "#**Ans-6**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGlTQS--3Szl",
        "outputId": "079b9d1b-ebbc-4850-9a69-d22fe3ce508c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Hyperparameters: {'C': 0.1, 'penalty': 'l2'}\n",
            "Best Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
            "20 fits failed out of a total of 40.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "20 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 63, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [nan  1. nan  1. nan  1. nan  1.]\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/demodata.csv')  # Replace with your dataset file\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = data.drop('target', axis=1)  # Replace 'target_variable' with the actual target column name\n",
        "y = data['target']\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],  # Values for C\n",
        "    'penalty': ['l1', 'l2']  # Values for penalty\n",
        "}\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Create GridSearchCV object\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')  # cv=5 for 5-fold cross-validation\n",
        "\n",
        "# Fit the GridSearchCV object to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "\n",
        "# Print the best accuracy score\n",
        "print(\"Best Accuracy:\", grid_search.best_score_)\n",
        "\n",
        "# Evaluate the model with the best hyperparameters on the testing set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsH6NjmwZ8Sq"
      },
      "source": [
        "#Que-7 Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy.\n",
        "#**Ans-7**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HZjCmH13cRS",
        "outputId": "ba3208b8-626b-4975-bf76-36f34a484398"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Accuracy: 0.8109999999999999\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "np.random.seed(0)\n",
        "n_samples = 1000\n",
        "data = pd.DataFrame({\n",
        "    'customer_age': np.random.randint(20, 70, n_samples),\n",
        "    'monthly_bill': np.random.uniform(20, 150, n_samples),\n",
        "    'data_usage': np.random.uniform(0, 100, n_samples),\n",
        "    'contract_length': np.random.randint(1, 36, n_samples),\n",
        "    'online_support': np.random.randint(0, 2, n_samples)  # 0: No, 1: Yes\n",
        "})\n",
        "# Simulate churn based on some conditions (this is just example logic)\n",
        "data['churn'] = (data['monthly_bill'] > 100) | (data['contract_length'] < 6) | (data['data_usage'] > 80)\n",
        "data['churn'] = data['churn'].astype(int) # Convert True/False to 1/0\n",
        "\n",
        "\n",
        "# 2. Prepare data (features and target)\n",
        "X = data[['customer_age', 'monthly_bill', 'data_usage', 'contract_length', 'online_support']]\n",
        "y = data['churn']\n",
        "\n",
        "# Create StratifiedKFold object\n",
        "skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)  # Adjust n_splits and random_state as needed\n",
        "\n",
        "# Initialize lists to store accuracy scores for each fold\n",
        "accuracy_scores = []\n",
        "\n",
        "# Iterate over the folds\n",
        "for train_index, test_index in skf.split(X, y):\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    # Create and train the Logistic Regression model\n",
        "    model = LogisticRegression()\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the testing set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy and append to the list\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracy_scores.append(accuracy)\n",
        "\n",
        "# Calculate and print the average accuracy\n",
        "average_accuracy = sum(accuracy_scores) / len(accuracy_scores)\n",
        "print(f\"Average Accuracy: {average_accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTf7JC9QZ8Y6"
      },
      "source": [
        "#Que-8 Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy.\n",
        "#**Ans-8**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tArQBnv3nDc",
        "outputId": "a9d7bede-5bb3-4c16-8b08-4dd2ad225ca5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Accuracy: 0.81\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "np.random.seed(0)\n",
        "n_samples = 1000\n",
        "data = pd.DataFrame({\n",
        "    'customer_age': np.random.randint(20, 70, n_samples),\n",
        "    'monthly_bill': np.random.uniform(20, 150, n_samples),\n",
        "    'data_usage': np.random.uniform(0, 100, n_samples),\n",
        "    'contract_length': np.random.randint(1, 36, n_samples),\n",
        "    'online_support': np.random.randint(0, 2, n_samples)  # 0: No, 1: Yes\n",
        "})\n",
        "# Simulate churn based on some conditions (this is just example logic)\n",
        "data['churn'] = (data['monthly_bill'] > 100) | (data['contract_length'] < 6) | (data['data_usage'] > 80)\n",
        "data['churn'] = data['churn'].astype(int) # Convert True/False to 1/0\n",
        "\n",
        "\n",
        "# 2. Prepare data (features and target)\n",
        "X = data[['customer_age', 'monthly_bill', 'data_usage', 'contract_length', 'online_support']]\n",
        "y = data['churn']\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Adjust test_size and random_state as needed\n",
        "\n",
        "# Create and train the Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gu--_L1bZ8fa"
      },
      "source": [
        "#Que-9 Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy.\n",
        "#**Ans-9**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PaD1oOs3zIK",
        "outputId": "d7633198-e8ed-4bc9-cca5-f40aebd06581"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Hyperparameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 1438.44988828766}\n",
            "Best Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/demodata.csv')  # Replace with your dataset file\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = data.drop('target', axis=1)  # Replace 'target_variable' with the actual target column name\n",
        "y = data['target']\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the hyperparameter distribution\n",
        "param_dist = {\n",
        "    'C': np.logspace(-4, 4, 20),  # Values for C (logarithmic scale)\n",
        "    'penalty': ['l1', 'l2'],  # Values for penalty\n",
        "    'solver': ['liblinear', 'saga']  # Values for solver (compatible with both penalties)\n",
        "}\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Create RandomizedSearchCV object\n",
        "random_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=10, cv=5, scoring='accuracy', random_state=42)\n",
        "\n",
        "# Fit the RandomizedSearchCV object to the training data\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
        "\n",
        "# Print the best accuracy score\n",
        "print(\"Best Accuracy:\", random_search.best_score_)\n",
        "\n",
        "# Evaluate the model with the best hyperparameters on the testing set\n",
        "best_model = random_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emoRWrAWZ8lk"
      },
      "source": [
        "#Que-10 Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy.\n",
        "#**Ans-10**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33bSHaUW37nA",
        "outputId": "8d4930e2-3b0b-4813-e45c-bdf469a80a0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "One-vs-One (OvO) Accuracy: 0.31333333333333335\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "n_samples = 1500  # Total number of samples\n",
        "n_features = 7    # Number of features\n",
        "n_classes = 3     # Number of classes for multiclass target variable\n",
        "# features (randomly generated for simplicity)\n",
        "np.random.seed(42) # for reproducibility\n",
        "features = np.random.randn(n_samples, n_features)\n",
        "feature_names = [f'feature_{i}' for i in range(n_features)]\n",
        "# Create multiclass target variable\n",
        "target = np.random.randint(0, n_classes, n_samples) # Randomly assign classes 0, 1, or 2\n",
        "# Pandas DataFrame\n",
        "df = pd.DataFrame(data=features, columns=feature_names)\n",
        "df['target'] = target.astype(int) # Ensure target is integer type\n",
        "df.to_csv('multiclass_dataset.csv', index=False)\n",
        "\n",
        "\n",
        "# Load the dataset (assuming it has a multiclass target variable)\n",
        "data = pd.read_csv('multiclass_dataset.csv')  # Replace with your dataset file\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = data.drop('target', axis=1)  # Replace 'target_variable' with the actual target column name\n",
        "y = data['target']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a base Logistic Regression model\n",
        "base_model = LogisticRegression()\n",
        "\n",
        "# Create OneVsOneClassifier with the base model\n",
        "ovo_model = OneVsOneClassifier(base_model)\n",
        "\n",
        "# Train the OvO model\n",
        "ovo_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = ovo_model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"One-vs-One (OvO) Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjdGYfbgZ8rj"
      },
      "source": [
        "#Que-11 Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification.\n",
        "#**Ans-11**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "ZEwAdITa4CaJ",
        "outputId": "7ca02bfa-dc8b-41ae-f5be-86c6c219e1df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 1.0\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset (assuming it has a binary target variable)\n",
        "data = pd.read_csv('/content/demodata.csv')  # Replace with your dataset file\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = data.drop('target', axis=1)  # Replace 'target_variable' with the actual target column name\n",
        "y = data['target']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Create confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize confusion matrix using seaborn heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=['Class 0', 'Class 1'],\n",
        "            yticklabels=['Class 0', 'Class 1'])\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZtnX_YzZ8x5"
      },
      "source": [
        "#Que-12 Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score.\n",
        "#**Ans-12**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxwsoCQm4U1w",
        "outputId": "2af5a373-55b2-41bb-95b7-e88c73227423"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision: 0.8771929824561403\n",
            "Recall: 0.8064516129032258\n",
            "F1-Score: 0.8403361344537815\n",
            "Accuracy: 0.81\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "np.random.seed(0)\n",
        "n_samples = 1000\n",
        "data = pd.DataFrame({\n",
        "    'customer_age': np.random.randint(20, 70, n_samples),\n",
        "    'monthly_bill': np.random.uniform(20, 150, n_samples),\n",
        "    'data_usage': np.random.uniform(0, 100, n_samples),\n",
        "    'contract_length': np.random.randint(1, 36, n_samples),\n",
        "    'online_support': np.random.randint(0, 2, n_samples)  # 0: No, 1: Yes\n",
        "})\n",
        "# Simulate churn based on some conditions (this is just example logic)\n",
        "data['churn'] = (data['monthly_bill'] > 100) | (data['contract_length'] < 6) | (data['data_usage'] > 80)\n",
        "data['churn'] = data['churn'].astype(int) # Convert True/False to 1/0\n",
        "\n",
        "\n",
        "# 2. Prepare data (features and target)\n",
        "X = data[['customer_age', 'monthly_bill', 'data_usage', 'contract_length', 'online_support']]\n",
        "y = data['churn']\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Precision, Recall, and F1-Score\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-Score: {f1}\")\n",
        "print(f\"Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsRfLyAjZ84i"
      },
      "source": [
        "#Que-13 Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance.\n",
        "#**Ans-13**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3ZGQjd94jJh",
        "outputId": "37363987-4916-49fc-be3e-494dce6ee513"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.585\n",
            "Precision: 0.15853658536585366\n",
            "Recall: 0.48148148148148145\n",
            "F1-Score: 0.23853211009174313\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "# 1. Set dataset parameters\n",
        "n_samples = 1000  # Total number of samples\n",
        "n_features = 5    # Number of features\n",
        "imbalance_ratio = 0.9 # Ratio of majority class (0) to minority class (1). 0.9 means 90% class 0, 10% class 1\n",
        "\n",
        "# 2. Create features (randomly generated for simplicity)\n",
        "np.random.seed(42) # for reproducibility\n",
        "features = np.random.randn(n_samples, n_features)\n",
        "feature_names = [f'feature_{i}' for i in range(n_features)]\n",
        "\n",
        "# 3. Create imbalanced target variable\n",
        "n_minority_class = int(n_samples * (1 - imbalance_ratio))\n",
        "n_majority_class = n_samples - n_minority_class\n",
        "\n",
        "target = np.concatenate([np.zeros(n_majority_class), np.ones(n_minority_class)])\n",
        "np.random.shuffle(target) # shuffle to mix classes\n",
        "\n",
        "# 4. Create Pandas DataFrame\n",
        "df = pd.DataFrame(data=features, columns=feature_names)\n",
        "df['target'] = target.astype(int) # Ensure target is integer type\n",
        "df.to_csv('imbalanced_dataset.csv', index=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load the imbalanced dataset\n",
        "data = pd.read_csv('imbalanced_dataset.csv')  # Replace with your dataset file\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = data.drop('target', axis=1)  # Replace 'target_variable' with the actual target column name\n",
        "y = data['target']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Logistic Regression model with class weights\n",
        "model = LogisticRegression(class_weight='balanced')  # Apply class weights\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-Score: {f1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-bZubqqZ8_D"
      },
      "source": [
        "#Que-14 Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance.\n",
        "#**Ans-14**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFeiLot34v0P",
        "outputId": "7b7eb81a-3c14-4f9b-8dc2-b28d6bc1c439"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8100558659217877\n",
            "Precision: 0.803030303030303\n",
            "Recall: 0.7162162162162162\n",
            "F1-Score: 0.7571428571428571\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load the Titanic dataset\n",
        "data = pd.read_csv('/content/Titanic-Dataset.csv')  # Replace 'titanic.csv' with the actual file path\n",
        "\n",
        "# Select relevant features and target variable\n",
        "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare']\n",
        "target = 'Survived'\n",
        "data = data[[target] + features]\n",
        "\n",
        "# Convert categorical features to numerical using one-hot encoding\n",
        "data = pd.get_dummies(data, columns=['Sex'], drop_first=True)\n",
        "\n",
        "# Handle missing values using imputation\n",
        "imputer = SimpleImputer(strategy='mean')  # Replace with other strategies if needed\n",
        "data[['Age']] = imputer.fit_transform(data[['Age']])\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = data.drop(columns=[target])\n",
        "y = data[target]\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-Score: {f1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gddxS9PNZ9Fh"
      },
      "source": [
        "#Que-15 Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling.\n",
        "#**Ans-15**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hk1PASme45rG",
        "outputId": "480f24fd-7f66-40b5-dd79-1733c4cb036a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy without scaling: 0.81\n",
            "Accuracy with scaling: 0.81\n",
            "Difference in accuracy: 0.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "np.random.seed(0)\n",
        "n_samples = 1000\n",
        "data = pd.DataFrame({\n",
        "    'customer_age': np.random.randint(20, 70, n_samples),\n",
        "    'monthly_bill': np.random.uniform(20, 150, n_samples),\n",
        "    'data_usage': np.random.uniform(0, 100, n_samples),\n",
        "    'contract_length': np.random.randint(1, 36, n_samples),\n",
        "    'online_support': np.random.randint(0, 2, n_samples)  # 0: No, 1: Yes\n",
        "})\n",
        "# Simulate churn based on some conditions (this is just example logic)\n",
        "data['churn'] = (data['monthly_bill'] > 100) | (data['contract_length'] < 6) | (data['data_usage'] > 80)\n",
        "data['churn'] = data['churn'].astype(int) # Convert True/False to 1/0\n",
        "\n",
        "\n",
        "# 2. Prepare data (features and target)\n",
        "X = data[['customer_age', 'monthly_bill', 'data_usage', 'contract_length', 'online_support']]\n",
        "y = data['churn']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train and evaluate without scaling\n",
        "model_no_scaling = LogisticRegression()\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "print(f\"Accuracy without scaling: {accuracy_no_scaling}\")\n",
        "\n",
        "# Apply feature scaling (Standardization)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train and evaluate with scaling\n",
        "model_scaling = LogisticRegression()\n",
        "model_scaling.fit(X_train_scaled, y_train)\n",
        "y_pred_scaling = model_scaling.predict(X_test_scaled)\n",
        "accuracy_scaling = accuracy_score(y_test, y_pred_scaling)\n",
        "print(f\"Accuracy with scaling: {accuracy_scaling}\")\n",
        "\n",
        "# Compare results\n",
        "print(f\"Difference in accuracy: {accuracy_scaling - accuracy_no_scaling}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1HR-fXAZ9Mc"
      },
      "source": [
        "#Que-16 Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score.\n",
        "#**Ans-16**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0wjvdp75DuO",
        "outputId": "dbc78158-39db-410b-fec8-c25d488f348b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROC-AUC Score: 0.8771222410865874\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load the dataset\n",
        "np.random.seed(0)\n",
        "n_samples = 1000\n",
        "data = pd.DataFrame({\n",
        "    'customer_age': np.random.randint(20, 70, n_samples),\n",
        "    'monthly_bill': np.random.uniform(20, 150, n_samples),\n",
        "    'data_usage': np.random.uniform(0, 100, n_samples),\n",
        "    'contract_length': np.random.randint(1, 36, n_samples),\n",
        "    'online_support': np.random.randint(0, 2, n_samples)  # 0: No, 1: Yes\n",
        "})\n",
        "# Simulate churn based on some conditions (this is just example logic)\n",
        "data['churn'] = (data['monthly_bill'] > 100) | (data['contract_length'] < 6) | (data['data_usage'] > 80)\n",
        "data['churn'] = data['churn'].astype(int) # Convert True/False to 1/0\n",
        "\n",
        "\n",
        "# 2. Prepare data (features and target)\n",
        "X = data[['customer_age', 'monthly_bill', 'data_usage', 'contract_length', 'online_support']]\n",
        "y = data['churn']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities on the testing set\n",
        "y_probs = model.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class\n",
        "\n",
        "# Calculate ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_probs)\n",
        "\n",
        "# Print the ROC-AUC score\n",
        "print(f\"ROC-AUC Score: {roc_auc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbXGlKpIZ9f5"
      },
      "source": [
        "#Que-17 Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy.\n",
        "#**Ans-17**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUW-FwYs5M6_",
        "outputId": "baf708f9-6fa3-4459-fbdd-c5abbb2f79bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy with custom learning rate (C=0.5): 0.81\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "np.random.seed(0)\n",
        "n_samples = 1000\n",
        "data = pd.DataFrame({\n",
        "    'customer_age': np.random.randint(20, 70, n_samples),\n",
        "    'monthly_bill': np.random.uniform(20, 150, n_samples),\n",
        "    'data_usage': np.random.uniform(0, 100, n_samples),\n",
        "    'contract_length': np.random.randint(1, 36, n_samples),\n",
        "    'online_support': np.random.randint(0, 2, n_samples)  # 0: No, 1: Yes\n",
        "})\n",
        "# Simulate churn based on some conditions (this is just example logic)\n",
        "data['churn'] = (data['monthly_bill'] > 100) | (data['contract_length'] < 6) | (data['data_usage'] > 80)\n",
        "data['churn'] = data['churn'].astype(int) # Convert True/False to 1/0\n",
        "\n",
        "\n",
        "# 2. Prepare data (features and target)\n",
        "X = data[['customer_age', 'monthly_bill', 'data_usage', 'contract_length', 'online_support']]\n",
        "y = data['churn']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Logistic Regression model with custom learning rate (C)\n",
        "model = LogisticRegression(C=0.5)  # Set C to 0.5 for the custom learning rate\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f\"Accuracy with custom learning rate (C=0.5): {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z12vW8RUZ9nD"
      },
      "source": [
        "#Que-18 Write a Python program to train Logistic Regression and identify important features based on model coefficients.\n",
        "#**Ans-18**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3VTPbb_5XEN",
        "outputId": "09d2e7b6-41b0-4cd4-e6c8-12332803e434"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "online_support: -0.11970701434263537\n",
            "contract_length: -0.07242010518672416\n",
            "monthly_bill: 0.057143468410568596\n",
            "data_usage: 0.04248680045211942\n",
            "customer_age: 0.004802538926286238\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load the dataset\n",
        "np.random.seed(0)\n",
        "n_samples = 1000\n",
        "data = pd.DataFrame({\n",
        "    'customer_age': np.random.randint(20, 70, n_samples),\n",
        "    'monthly_bill': np.random.uniform(20, 150, n_samples),\n",
        "    'data_usage': np.random.uniform(0, 100, n_samples),\n",
        "    'contract_length': np.random.randint(1, 36, n_samples),\n",
        "    'online_support': np.random.randint(0, 2, n_samples)  # 0: No, 1: Yes\n",
        "})\n",
        "# Simulate churn based on some conditions (this is just example logic)\n",
        "data['churn'] = (data['monthly_bill'] > 100) | (data['contract_length'] < 6) | (data['data_usage'] > 80)\n",
        "data['churn'] = data['churn'].astype(int) # Convert True/False to 1/0\n",
        "\n",
        "\n",
        "# 2. Prepare data (features and target)\n",
        "X = data[['customer_age', 'monthly_bill', 'data_usage', 'contract_length', 'online_support']]\n",
        "y = data['churn']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importance based on coefficients\n",
        "importance = model.coef_[0]\n",
        "\n",
        "# Create a dictionary of feature names and their importance\n",
        "feature_importance = dict(zip(X.columns, importance))\n",
        "\n",
        "# Print feature importance\n",
        "for feature, importance_score in sorted(feature_importance.items(), key=lambda item: abs(item[1]), reverse=True):\n",
        "    print(f\"{feature}: {importance_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCuXtjZ5Z9sh"
      },
      "source": [
        "#Que-19 Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa Score.\n",
        "#**Ans-19**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7Vzo9l95irE",
        "outputId": "90c78469-b4d9-4ee7-ec52-1b92b78f5998"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cohen's Kappa Score: 0.6067880794701987\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "# Load the dataset\n",
        "np.random.seed(0)\n",
        "n_samples = 1000\n",
        "data = pd.DataFrame({\n",
        "    'customer_age': np.random.randint(20, 70, n_samples),\n",
        "    'monthly_bill': np.random.uniform(20, 150, n_samples),\n",
        "    'data_usage': np.random.uniform(0, 100, n_samples),\n",
        "    'contract_length': np.random.randint(1, 36, n_samples),\n",
        "    'online_support': np.random.randint(0, 2, n_samples)  # 0: No, 1: Yes\n",
        "})\n",
        "# Simulate churn based on some conditions (this is just example logic)\n",
        "data['churn'] = (data['monthly_bill'] > 100) | (data['contract_length'] < 6) | (data['data_usage'] > 80)\n",
        "data['churn'] = data['churn'].astype(int) # Convert True/False to 1/0\n",
        "\n",
        "\n",
        "# 2. Prepare data (features and target)\n",
        "X = data[['customer_age', 'monthly_bill', 'data_usage', 'contract_length', 'online_support']]\n",
        "y = data['churn']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Cohen's Kappa Score\n",
        "kappa = cohen_kappa_score(y_test, y_pred)\n",
        "\n",
        "# Print the Cohen's Kappa Score\n",
        "print(f\"Cohen's Kappa Score: {kappa}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hjooBbMZ9yr"
      },
      "source": [
        "#Que-20 Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classificatio:\n",
        "#**Ans-20**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "HHGtnXgl5sIs",
        "outputId": "3b82adde-62a1-45a0-be2a-d4136ffce198"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "\n",
        "# Load the dataset\n",
        "np.random.seed(0)\n",
        "n_samples = 1000\n",
        "data = pd.DataFrame({\n",
        "    'customer_age': np.random.randint(20, 70, n_samples),\n",
        "    'monthly_bill': np.random.uniform(20, 150, n_samples),\n",
        "    'data_usage': np.random.uniform(0, 100, n_samples),\n",
        "    'contract_length': np.random.randint(1, 36, n_samples),\n",
        "    'online_support': np.random.randint(0, 2, n_samples)  # 0: No, 1: Yes\n",
        "})\n",
        "# Simulate churn based on some conditions (this is just example logic)\n",
        "data['churn'] = (data['monthly_bill'] > 100) | (data['contract_length'] < 6) | (data['data_usage'] > 80)\n",
        "data['churn'] = data['churn'].astype(int) # Convert True/False to 1/0\n",
        "\n",
        "\n",
        "# 2. Prepare data (features and target)\n",
        "X = data[['customer_age', 'monthly_bill', 'data_usage', 'contract_length', 'online_support']]\n",
        "y = data['churn']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities on the testing set\n",
        "y_probs = model.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class\n",
        "\n",
        "# Calculate precision and recall\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_probs)\n",
        "\n",
        "# Calculate average precision score\n",
        "average_precision = average_precision_score(y_test, y_probs)\n",
        "\n",
        "# Plot the Precision-Recall curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.step(recall, precision, color='b', alpha=0.2, where='post')\n",
        "plt.fill_between(recall, precision, step='post', alpha=0.2, color='b')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.title(f'Precision-Recall curve: AP={average_precision:.2f}')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sEzAgl4Z94x"
      },
      "source": [
        "#Que-21 Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy.\n",
        "#**Ans-21**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXWNyUS85--7",
        "outputId": "135010f2-6089-4d5e-f491-3ab7543821e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy with solver 'liblinear': 0.8\n",
            "Accuracy with solver 'saga': 0.75\n",
            "Accuracy with solver 'lbfgs': 0.81\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "np.random.seed(0)\n",
        "n_samples = 1000\n",
        "data = pd.DataFrame({\n",
        "    'customer_age': np.random.randint(20, 70, n_samples),\n",
        "    'monthly_bill': np.random.uniform(20, 150, n_samples),\n",
        "    'data_usage': np.random.uniform(0, 100, n_samples),\n",
        "    'contract_length': np.random.randint(1, 36, n_samples),\n",
        "    'online_support': np.random.randint(0, 2, n_samples)  # 0: No, 1: Yes\n",
        "})\n",
        "# Simulate churn based on some conditions (this is just example logic)\n",
        "data['churn'] = (data['monthly_bill'] > 100) | (data['contract_length'] < 6) | (data['data_usage'] > 80)\n",
        "data['churn'] = data['churn'].astype(int) # Convert True/False to 1/0\n",
        "\n",
        "\n",
        "# 2. Prepare data (features and target)\n",
        "X = data[['customer_age', 'monthly_bill', 'data_usage', 'contract_length', 'online_support']]\n",
        "y = data['churn']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define a list of solvers to compare\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "\n",
        "# Train and evaluate Logistic Regression with each solver\n",
        "for solver in solvers:\n",
        "    model = LogisticRegression(solver=solver)  # Specify the solver\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Accuracy with solver '{solver}': {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9shbvThiZ9_6"
      },
      "source": [
        "#Que-22 Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC).\n",
        "#**Ans-22**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRsX3b8c6KbS",
        "outputId": "f97c7d87-7eb2-4eb0-d522-eff13599594e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matthews Correlation Coefficient (MCC): 0.6100629797190257\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "# Load the dataset\n",
        "np.random.seed(0)\n",
        "n_samples = 1000\n",
        "data = pd.DataFrame({\n",
        "    'customer_age': np.random.randint(20, 70, n_samples),\n",
        "    'monthly_bill': np.random.uniform(20, 150, n_samples),\n",
        "    'data_usage': np.random.uniform(0, 100, n_samples),\n",
        "    'contract_length': np.random.randint(1, 36, n_samples),\n",
        "    'online_support': np.random.randint(0, 2, n_samples)  # 0: No, 1: Yes\n",
        "})\n",
        "# Simulate churn based on some conditions (this is just example logic)\n",
        "data['churn'] = (data['monthly_bill'] > 100) | (data['contract_length'] < 6) | (data['data_usage'] > 80)\n",
        "data['churn'] = data['churn'].astype(int) # Convert True/False to 1/0\n",
        "\n",
        "\n",
        "# 2. Prepare data (features and target)\n",
        "X = data[['customer_age', 'monthly_bill', 'data_usage', 'contract_length', 'online_support']]\n",
        "y = data['churn']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Matthews Correlation Coefficient (MCC)\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "\n",
        "# Print the MCC score\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeccjHh6Z-G5"
      },
      "source": [
        "#Que-23 Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling.\n",
        "#**Ans-23**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjixtKBE6TtL",
        "outputId": "47b18f21-b4c3-4399-f49c-95f5eaf60a60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on raw data: 0.81\n",
            "Accuracy on standardized data: 0.81\n",
            "Difference in accuracy: 0.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "np.random.seed(0)\n",
        "n_samples = 1000\n",
        "data = pd.DataFrame({\n",
        "    'customer_age': np.random.randint(20, 70, n_samples),\n",
        "    'monthly_bill': np.random.uniform(20, 150, n_samples),\n",
        "    'data_usage': np.random.uniform(0, 100, n_samples),\n",
        "    'contract_length': np.random.randint(1, 36, n_samples),\n",
        "    'online_support': np.random.randint(0, 2, n_samples)  # 0: No, 1: Yes\n",
        "})\n",
        "# Simulate churn based on some conditions (this is just example logic)\n",
        "data['churn'] = (data['monthly_bill'] > 100) | (data['contract_length'] < 6) | (data['data_usage'] > 80)\n",
        "data['churn'] = data['churn'].astype(int) # Convert True/False to 1/0\n",
        "\n",
        "\n",
        "# 2. Prepare data (features and target)\n",
        "X = data[['customer_age', 'monthly_bill', 'data_usage', 'contract_length', 'online_support']]\n",
        "y = data['churn']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train and evaluate on raw data\n",
        "model_raw = LogisticRegression()\n",
        "model_raw.fit(X_train, y_train)\n",
        "y_pred_raw = model_raw.predict(X_test)\n",
        "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
        "print(f\"Accuracy on raw data: {accuracy_raw}\")\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train and evaluate on standardized data\n",
        "model_scaled = LogisticRegression()\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "print(f\"Accuracy on standardized data: {accuracy_scaled}\")\n",
        "\n",
        "# Compare accuracy\n",
        "print(f\"Difference in accuracy: {accuracy_scaled - accuracy_raw}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNltgShlZ-NZ"
      },
      "source": [
        "#Que-24 Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation.\n",
        "#**Ans-24**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4W2VnD16ePZ",
        "outputId": "8d3cf435-c5b9-445d-cad5-4b86d309f0ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best C: 0.001\n",
            "Best Accuracy: 0.8125\n",
            "Accuracy on testing set: 0.81\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "np.random.seed(0)\n",
        "n_samples = 1000\n",
        "data = pd.DataFrame({\n",
        "    'customer_age': np.random.randint(20, 70, n_samples),\n",
        "    'monthly_bill': np.random.uniform(20, 150, n_samples),\n",
        "    'data_usage': np.random.uniform(0, 100, n_samples),\n",
        "    'contract_length': np.random.randint(1, 36, n_samples),\n",
        "    'online_support': np.random.randint(0, 2, n_samples)  # 0: No, 1: Yes\n",
        "})\n",
        "# Simulate churn based on some conditions (this is just example logic)\n",
        "data['churn'] = (data['monthly_bill'] > 100) | (data['contract_length'] < 6) | (data['data_usage'] > 80)\n",
        "data['churn'] = data['churn'].astype(int) # Convert True/False to 1/0\n",
        "\n",
        "\n",
        "# 2. Prepare data (features and target)\n",
        "X = data[['customer_age', 'monthly_bill', 'data_usage', 'contract_length', 'online_support']]\n",
        "y = data['churn']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the hyperparameter grid for C\n",
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Create GridSearchCV object\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')  # cv=5 for 5-fold cross-validation\n",
        "\n",
        "# Fit GridSearchCV to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best C value and the corresponding accuracy\n",
        "best_C = grid_search.best_params_['C']\n",
        "best_accuracy = grid_search.best_score_\n",
        "\n",
        "# Print the best C value and accuracy\n",
        "print(f\"Best C: {best_C}\")\n",
        "print(f\"Best Accuracy: {best_accuracy}\")\n",
        "\n",
        "# Train the final model with the best C value\n",
        "final_model = LogisticRegression(C=best_C)\n",
        "final_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the final model on the testing set\n",
        "y_pred = final_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy on testing set: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdnnQjKLZ-VD"
      },
      "source": [
        "#Que-25 Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions.\n",
        "#**Ans-25**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFy_T8v36oTZ",
        "outputId": "d3406902-9a72-4d1f-8435-7cd650f7dfab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of loaded model: 0.81\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "\n",
        "# Load the dataset\n",
        "np.random.seed(0)\n",
        "n_samples = 1000\n",
        "data = pd.DataFrame({\n",
        "    'customer_age': np.random.randint(20, 70, n_samples),\n",
        "    'monthly_bill': np.random.uniform(20, 150, n_samples),\n",
        "    'data_usage': np.random.uniform(0, 100, n_samples),\n",
        "    'contract_length': np.random.randint(1, 36, n_samples),\n",
        "    'online_support': np.random.randint(0, 2, n_samples)  # 0: No, 1: Yes\n",
        "})\n",
        "# Simulate churn based on some conditions (this is just example logic)\n",
        "data['churn'] = (data['monthly_bill'] > 100) | (data['contract_length'] < 6) | (data['data_usage'] > 80)\n",
        "data['churn'] = data['churn'].astype(int) # Convert True/False to 1/0\n",
        "\n",
        "\n",
        "# 2. Prepare data (features and target)\n",
        "X = data[['customer_age', 'monthly_bill', 'data_usage', 'contract_length', 'online_support']]\n",
        "y = data['churn']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Save the trained model using joblib\n",
        "joblib.dump(model, 'logistic_regression_model.pkl')\n",
        "\n",
        "# Load the saved model\n",
        "loaded_model = joblib.load('logistic_regression_model.pkl')\n",
        "\n",
        "# Make predictions on the testing set using the loaded model\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f\"Accuracy of loaded model: {accuracy}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
